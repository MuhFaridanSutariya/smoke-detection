# -*- coding: utf-8 -*-
"""smoke detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19KFvYv9scEFUJ4CwuKQpfxAcQq9GBkQc
"""

# liabraries for data manipulation
import pandas as pd

# libraries for data visualization
from plotly.subplots import make_subplots
import plotly.graph_objs as go
import plotly.offline as py
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

# libraries for preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# libraries for modelling
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# libraries evaluation
from sklearn.metrics import recall_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV

# libraries for ignore warning after run code
import warnings
warnings.filterwarnings('ignore')

# import dataset dan menampilkan 5 teratas
df = pd.read_csv('/content/smoke_detection_iot.csv',index_col = False)
df.head()

# menghapus kolom yang tidak berguna
df = df.drop(columns='Unnamed: 0')

# melihat dimensi dari dataset
print("Row: {}, Columns: {}".format(df.shape[0], df.shape[1]))

"""# Exploratory data analysis

## Exploratory Data Analysis - Deskripsi Variabel:
"""

df.info

# melihat info dari dataset
info = []
col = df.columns.tolist()
for i in col:
  info.append(df[i].dtype)

info1 = pd.DataFrame(info, col, columns=['Type'])
info1

# melihat deskriptf statistik
df.describe()

"""## Exploratory Exploratory Data Analysis - Menangani Missing Value dan Outliers:"""

# melihat missing value
Total = df.isnull().sum().sort_values(ascending=False)          

Percent = (df.isnull().sum()*100/df.isnull().count()).sort_values(ascending=False)   

missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    
missing_data

# melihat outliers
cols = df.columns
for i, col in enumerate(cols):
  sns.boxplot(x=df[col])
  plt.show()

"""## Exploratory Data Analysis - Univariate Analysis:"""

# melihat proporsi pada feature target
trace = go.Pie(labels = ['Yes_Fire', 'No_Fire'], values = df['Fire Alarm'].value_counts(), 
               textfont=dict(size=15), opacity = 0.8,
               marker=dict(colors=['ldarkblue','orange'], 
                           line=dict(color='#000000', width=1.5)))


layout = dict(title =  'Distribution of Fire Alarm variable')
           
fig = dict(data = [trace], layout=layout)
py.iplot(fig)

"""## Exploratory Data Analysis - Multivariate Analysis:"""

# melihat hubungan humidity dengan variabel target
plt.figure(figsize=(15,5))
sns.kdeplot( data=df, x='Humidity[%]', hue='Fire Alarm', fill = True)
plt.legend(loc='upper left', labels=['no fire', 'yes fire'])
plt.title('Humidity vs Fire density')
plt.show()

# melihat hubungan temperatur dengan variabel target
plt.figure(figsize=(15,5))
sns.kdeplot( data=df, x='Temperature[C]', hue='Fire Alarm', fill = True)
plt.legend(loc='upper left', labels=['no fire', 'yes fire'])
plt.title('Temperature vs Fire density')
plt.show()

# melihat hubungan pressure dengan variabel target
plt.figure(figsize=(15,5))
sns.kdeplot( data=df, x='Pressure[hPa]', hue='Fire Alarm', fill = True)
plt.legend(loc='upper left', labels=['no fire', 'yes fire'])
plt.title('Pressure vs Fire density')
plt.show()

# melihat hubungan Raw H2 dengan variabel target
plt.figure(figsize=(15,5))
sns.kdeplot( data=df, x='Raw H2', hue='Fire Alarm', fill = True)
plt.legend(loc='upper left', labels=['no fire', 'yes fire'])
plt.title('Raw H2 vs Fire density')
plt.show()

# melihat hubungan Raw Ethanol dengan variabel target
plt.figure(figsize=(15,5))
sns.kdeplot( data=df, x='Raw Ethanol', hue='Fire Alarm', fill = True)
plt.legend(loc='upper left', labels=['no fire', 'yes fire'])
plt.title('Raw Ethanol vs Fire density')
plt.show()

# melihat korelasi dari tiap feature yang ada
plt.figure(figsize = (12,12))
sns.heatmap(df.corr(),annot = True,cmap = 'GnBu')
plt.show()

"""# Data Preparation

## Feature Selection

Fitur yang sangat didominasi oleh salah satu nilai saja akan dibuang pada tahap ini.
"""

for col in df.columns.tolist():
    print(df[col].value_counts().count())
    print('\n')

"""menghapus kolom UTC karena tidak berpengaruh pada model machine learning"""

df = df.drop(columns='UTC')
df.head()

"""## Principal Component Analysis (PCA)

Disini kita akan lakukan PCA untuk mereduksi feature yang ada sehingga model dapat dengan mudah mendapatkan pola dari dataset kita.

feature yang akan kita lakukan PCA adalah PM1.0, PM2.5, NC0.5, NC1.0 dan NC2.5 karena pada feature tersebut memiliki korelasi positif yang tinggi
"""

pca = PCA(n_components=5, random_state=42)
pca.fit(df[['PM1.0', 'PM2.5', 'NC0.5', 'NC1.0','NC2.5']])
princ_comp = pca.transform(df[['PM1.0', 'PM2.5', 'NC0.5', 'NC1.0','NC2.5']])

pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=42)
pca.fit(df[['PM1.0', 'PM2.5', 'NC0.5', 'NC1.0','NC2.5']])
df['dimension'] = pca.transform(df.loc[:, ('PM1.0', 'PM2.5', 'NC0.5', 'NC1.0','NC2.5')]).flatten()
df.drop(['PM1.0', 'PM2.5', 'NC0.5', 'NC1.0','NC2.5'], axis=1, inplace=True)
df.head()

"""## Pembagian dataset"""

# splitting dataset
X = df.drop(["Fire Alarm"],axis =1)
y = df["Fire Alarm"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

# scaling data
scaling = StandardScaler()
X_train = scaling.fit_transform(X_train)

"""# Model development

Kita akan lakukan training pada data train dan melakukan predict pada data test yang telah kita split sebelumnya. Algoritma yang akan kita gunakan adalah Logistic Regression
"""

model = LogisticRegression()
solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2']
c_values = [100, 10, 1.0, 0.1, 0.01]

# define grid search
grid = dict(solver=solvers,penalty=penalty,C=c_values)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=3, scoring='recall')
grid_result = grid_search.fit(X, y)

"""# Evaluation

Menampilkan hasil dari prediksi berupa score dari setiap kombinasi parameter yang kita lakukan tuning sebelumnya.

Menampilkan confusion matrix
"""

best_random_grid = grid_result.best_estimator_
y_pred=best_random_grid.predict(X_test)
cf_matrix = confusion_matrix(y_test,y_pred)
print(cf_matrix)
print("Recall Score {}".format(recall_score(y_test,y_pred)))
print(classification_report(y_test,y_pred))

sns.heatmap(cf_matrix, annot=True, fmt='.5g');

# summarize results
print("%f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, param in zip(means, params):
    print("%f with: %r" % (mean, param))